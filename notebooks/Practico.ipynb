{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a49e9927-f016-4660-89df-26c5800aea8b",
   "metadata": {},
   "source": [
    "# Práctico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe2ae9e-6076-4a12-931c-5349ee7eb2d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "El trabajo práctico de la materia consiste en el análisis de un conjunto de datos extraído de Twitter. La idea es emplear los conceptos de grafos vistos en clase sobre un caso real de actualidad.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "El dataset consiste en un conjunto de hilos de tweets, con un total de ~150000 tweets, extraídos entre Enero y Marzo de 2021. La temática de los mismos está referida a la vacunación contra el covid-19 en Argentina.\n",
    "\n",
    "Pueden descargar el dataset del siguiente [link](https://drive.google.com/file/d/1X_qKsE8muAnom2tDX4sLlmBAO0Ikfe_G/view?usp=sharing).\n",
    "\n",
    "### Campos\n",
    "\n",
    "- **created_at:** Fecha del tweet\n",
    "- **id_str:** ID del tweet\n",
    "- **full_text:** Contenido del tweet\n",
    "- **in_reply_to_status_id:** ID del tweet inmediatamente anterior en el hilo\n",
    "- **in_reply_to_user_id:** Autor del tweet inmediatamente anterior en el hilo\n",
    "- **user.id:** Autor del tweet\n",
    "- **user_retweeters:** Lista de ID de usuarios que retweetearon el tweet\n",
    "- **sentiment:** Etiquetado manual que indica el sentimiento o intención del tweet con respecto al tweet anterior en el hilo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eed8c8b-8c40-4d18-b4ad-c853e1c3b7d1",
   "metadata": {},
   "source": [
    "## Configuración inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a23c83a7-0c4e-4bff-b8cb-253671e5d1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "## Descargar el csv con los datos en este directorio\n",
    "DATA_DIR = Path('../data/twitter')\n",
    "INPUT_FILE = DATA_DIR / 'vacunas.csv'\n",
    "\n",
    "## Creamos el directorio en caso de que no exista\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf6889f-86be-4fc8-b281-a738b8d475c4",
   "metadata": {},
   "source": [
    "### Cargamos el dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e43cef47-be3b-47ba-9a07-9373013580ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(155123, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id_str</th>\n",
       "      <th>full_text</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>user.id</th>\n",
       "      <th>user_retweeters</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sat Feb 20 03:09:10 +0000 2021</td>\n",
       "      <td>1362962469749153792</td>\n",
       "      <td>Seguimos esperando el comunicado de @norabar r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2737379453</td>\n",
       "      <td>[2258074658, 159909978, 105301854, 290671142, ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sat Feb 20 03:19:59 +0000 2021</td>\n",
       "      <td>1362965193509265417</td>\n",
       "      <td>@Clon_43 @norabar Nora estaba indignada porque...</td>\n",
       "      <td>1362962469749153792</td>\n",
       "      <td>2737379453</td>\n",
       "      <td>32718111</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mon Feb 22 23:55:08 +0000 2021</td>\n",
       "      <td>1364000806740111363</td>\n",
       "      <td>Bueno, Alberto dijo Salud o Economía. La salud...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>252168075</td>\n",
       "      <td>[1238117630696972289, 37232479, 12792246571247...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tue Feb 23 00:09:14 +0000 2021</td>\n",
       "      <td>1364004354374696963</td>\n",
       "      <td>@spitta1969 Tuit del mes Spitta</td>\n",
       "      <td>1364000806740111363</td>\n",
       "      <td>252168075</td>\n",
       "      <td>1156346340802224128</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tue Feb 23 00:00:17 +0000 2021</td>\n",
       "      <td>1364002100364128260</td>\n",
       "      <td>@spitta1969 Estas onfire</td>\n",
       "      <td>1364000806740111363</td>\n",
       "      <td>252168075</td>\n",
       "      <td>153663816</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at               id_str  \\\n",
       "0  Sat Feb 20 03:09:10 +0000 2021  1362962469749153792   \n",
       "1  Sat Feb 20 03:19:59 +0000 2021  1362965193509265417   \n",
       "2  Mon Feb 22 23:55:08 +0000 2021  1364000806740111363   \n",
       "3  Tue Feb 23 00:09:14 +0000 2021  1364004354374696963   \n",
       "4  Tue Feb 23 00:00:17 +0000 2021  1364002100364128260   \n",
       "\n",
       "                                           full_text in_reply_to_status_id  \\\n",
       "0  Seguimos esperando el comunicado de @norabar r...                   NaN   \n",
       "1  @Clon_43 @norabar Nora estaba indignada porque...   1362962469749153792   \n",
       "2  Bueno, Alberto dijo Salud o Economía. La salud...                   NaN   \n",
       "3                    @spitta1969 Tuit del mes Spitta   1364000806740111363   \n",
       "4                           @spitta1969 Estas onfire   1364000806740111363   \n",
       "\n",
       "  in_reply_to_user_id              user.id  \\\n",
       "0                 NaN           2737379453   \n",
       "1          2737379453             32718111   \n",
       "2                 NaN            252168075   \n",
       "3           252168075  1156346340802224128   \n",
       "4           252168075            153663816   \n",
       "\n",
       "                                     user_retweeters sentiment  \n",
       "0  [2258074658, 159909978, 105301854, 290671142, ...       NaN  \n",
       "1                                                 []       NaN  \n",
       "2  [1238117630696972289, 37232479, 12792246571247...       NaN  \n",
       "3                                                 []       NaN  \n",
       "4                                                 []       NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtypes = {\n",
    "    'id_str': str,\n",
    "    'full_text': str,\n",
    "    'in_reply_to_status_id': str,\n",
    "    'in_reply_to_user_id': str,\n",
    "    'user.id': str\n",
    "}\n",
    "df = pd.read_csv(INPUT_FILE, dtype=dtypes).dropna(subset=['user_retweeters'])\n",
    "df['user_retweeters'] = df['user_retweeters'].apply(lambda x: [str(elem) for elem in eval(x)])\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd778ff2-c52b-4541-83bb-ab515e233fb3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Observamos algunos ejemplos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "626fb5fa-2875-47c0-9e30-cb1a456a1c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto: Seguimos esperando el comunicado de @norabar repudiando la situación respecto del gobierno y el tema vacunas. Seamos pacientes que con esto de la pandemia anda con mucho \"laburo\".\n",
      "Retweets: 9\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "print('Texto:', df.full_text.values[idx])\n",
    "print('Retweets:', len(df.user_retweeters.values[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6ca9ab4-d966-4613-8d06-b139b30255b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Todo lo que hay que entender sobre la decisión –o no– de poner más vacunas en más brazos (por ejemplo, usar las 1º dosis en muchos y si es necesario retrasar la 2º) está en esta excelente nota de Nora Bär. https://t.co/A0I03DyxgO\n",
      "Retweets: 48\n"
     ]
    }
   ],
   "source": [
    "idx = 376\n",
    "print('Text:', df.full_text.values[idx])\n",
    "print('Retweets:', len(df.user_retweeters.values[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3911d7-cc49-4d84-95b6-4f096f5f56c3",
   "metadata": {},
   "source": [
    "### Calculamos la cantidad de hilos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59b6c11f-5766-45e4-a264-f3733df8694d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3174, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roots = df[df['in_reply_to_user_id'].isna()]\n",
    "roots.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9258ad30-e325-45ce-94e8-9e0ec4317df4",
   "metadata": {},
   "source": [
    "## Actividades"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f377d2-6e8d-437c-ab01-1a314b7af7eb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Primera parte\n",
    "\n",
    "#### **1. Construcción del grafo** \n",
    "\n",
    "Construir el **grafo de retweets**, definido de la siguiente manera:\n",
    "\n",
    "- Tipo de grafo: Dirigido\n",
    "- Nodos: ID de los usuarios\n",
    "- Enlaces: (Usuario A) ---> (Usuario B) si B retweeteó algún tweet de A\n",
    "\n",
    "Con estos datos, el grafo debería tener alrededor de 40000 nodos y 90000 enlaces.\n",
    "\n",
    "Considerar la versión no dirigida del grafo y estudiar su conectividad. Si existe una única \"componente gigante\", realizar el resto de las actividades sobre ella, en lugar de sobre el grafo completo.\n",
    "\n",
    "Calcular las siguientes métricas globales del grafo:\n",
    "\n",
    "- Grado medio\n",
    "- Asortatividad\n",
    "- Transitividad\n",
    "- Coeficiente de clustering de Watts-Strogatz\n",
    "\n",
    "**Opcional:** Comparar las métricas calculadas anteriormente con las de un grafo aleatorio con la misma distribución de grado. Pueden utilizar para ello este [método](https://networkx.org/documentation/stable/reference/generated/networkx.generators.degree_seq.configuration_model.html?highlight=configuration#networkx.generators.degree_seq.configuration_model). Con esto en mente, comentar si los valores obtenidos anteriormente difieren significativamente del caso aleatorio.\n",
    "\n",
    "\n",
    "#### **2. Centralidad**\n",
    "\n",
    "Calcular 5 métricas de centralidad de nodos. Graficar la distribución de cada una de ellas ¿Existe alguna correlación entre las distintas centralidades? \n",
    "\n",
    "Hacer un ranking con los 10 nodos más centrales para cada métrica. ¿Hay coincidencia entre los rankings?. ¿Qué características tienen los usuarios más centrales y sus respectivos tweets?\n",
    "\n",
    "**Opcional:** Determinar si existe alguna correlación entre la centralidad de un nodo y su actividad en red social. Es decir, evaluar si los usuarios que más escriben son los más centrales o no.\n",
    "\n",
    "#### **3. Comunidades**\n",
    "\n",
    "Utilizar el algoritmo de Louvain con el parámetro \"resolución\" igual a 1. Caracterizar las comunidades halladas (cantidad, distribución de tamaños). Utilizar la modularidad y otras dos métricas a elección para evaluar la calidad de la partición encontrada. \n",
    "\n",
    "Variar el parámetro \"resolución\" y observar cómo cambia la distribución de comunidades encontradas. ¿Existe algún valor para el cual se identifiquen dos grandes comunidades?\n",
    "\n",
    "Elegir otro algoritmo de detección de comunidades y comparar los resultados con los obtenidos anteriormente.\n",
    "\n",
    "**Opcional:** Correr el algoritmo de Louvain con distintas semillas aleatorias. Utilizar alguna métrica de comparación externa entre las particiones obtenidas para determinar en qué medida depende el algoritmo de la condición inicial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079e39e1-9b78-4b44-9743-604bf0ca5a3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Segunda parte\n",
    "\n",
    "### **4. Extracción de etiquetas**\n",
    "\n",
    "En el archivo [etiquetas.csv](https://drive.google.com/file/d/1X_qKsE8muAnom2tDX4sLlmBAO0Ikfe_G/view?usp=sharing) están las etiquetas para un pequeño subconjunto de nodos. Podemos interpretar el valor de la etiqueta como la pertenencia a una determinada clase, donde los usuarios de una misma clase en general tienden a expresar apoyo entre sí.\n",
    "\n",
    "- Determinar quiénes son los usuarios referentes de cada clase (utilizar alguna medida de centralidad calculada sobre el grafo de retweets).\n",
    "- Utiliando los resultados del práctico anterior, determinar si los usuarios de cada clase forman parte de distintas comunidades.\n",
    "\n",
    "**Opcional:** Reconstruir el archivo \"etiquetas.csv\". Para eso, hacer lo siguiente\n",
    "\n",
    "- Construir un grafo en donde los nodos sean usuarios, y donde los enlaces unan dos nodos si entre ellos hubo más respuestas de apoyo que de oposición.\n",
    "- Extraer las dos componentes más grandes del grafo. Esos serán nuestros nodos etiquetados.\n",
    "\n",
    "### **5. Embedding de nodos**\n",
    "\n",
    "- Generar un embedding del grafo de retweets utilizando el algoritmo `word2vec`.\n",
    "- Reducir a 2 la dimensionalidad del embedding utilizando [PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.htmlhttps://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) y [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.htmlhttps://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html).\n",
    "- Graficar los embeddings correspondientes a los datos etiquetados. ¿Es posible diferenciar unos de otros?\n",
    "\n",
    "**Opcional:** Graficar además los embeddings de los nodos que forman parte de las comunidades asociadas a cada clase. Determinar si el embedding permite distinguir cada comunidad.\n",
    "\n",
    "### **Opcional: 6. Redes neuronales de grafos**\n",
    "\n",
    "El archivo [word_vectors.csv](https://drive.google.com/file/d/1aoxugyMktKb0NQ8Pf3bdhKvh8BAj7YZz/view?usp=sharing) contiene un embedding de 300 dimensiones para cada tweet, otenido utilizando un modelo preentrenado de [FastText](https://fasttext.cc/). Construir una matriz de features para los nodos tomando, para cada usuario, el promedio de los vectores correspondientes a los tweets que escribió. Utilizando estos features, y tomando como ejemplos etiquetados los usuarios de \"etiquetas.csv\" entrenar una red neuronal de grafos para realizar una clasificación binaria sobre el resto de los nodos. Pueden utilizar como base el siguiente modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5333b57-a167-46a6-af83-95d1960921d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(1234)\n",
    "        self.conv1 = GCNConv(dataset.num_features, 4)\n",
    "        self.conv2 = GCNConv(4, 4)\n",
    "        self.conv3 = GCNConv(4, 2)\n",
    "        self.classifier = Linear(2, dataset.num_classes)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        h = self.conv1(x, edge_index)\n",
    "        h = h.tanh()\n",
    "        h = self.conv2(h, edge_index)\n",
    "        h = h.tanh()\n",
    "        h = self.conv3(h, edge_index)\n",
    "        h = h.tanh()  # Embedding final\n",
    "        \n",
    "        # Aplicamos un clasificador lineal sobre el embedding\n",
    "        out = self.classifier(h)\n",
    "\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319c67f8-f2aa-4b95-8f0a-9a6c8f28b221",
   "metadata": {},
   "source": [
    "**Observación:** para alimentar la red neuronal, es necesario construir un objeto de la clase `Dataset` de PyTorch-Geometric. Una forma de hacer eso es la siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc2f06b-eb86-4970-ba98-618267618521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "\n",
    "## Reemplazar por el grafo correspondiente\n",
    "g = nx.Graph()\n",
    "\n",
    "## Etiquetas. Reemplazar por las clases del archivo 'etiquetas.csv'.\n",
    "## Asignar la clase '2' a los ejemplos no etiquetados\n",
    "labels = [1, 0, 2, ..., 1]\n",
    "\n",
    "## True si el ejemplo está etiquetado (clases 0 y 1)\n",
    "train_idx = [True, True, False, ..., True]\n",
    "\n",
    "## Matriz de features (word vectors)\n",
    "features = ...\n",
    "\n",
    "adj = nx.to_scipy_sparse_matrix(g).tocoo()\n",
    "row = torch.from_numpy(adj.row.astype(np.int64)).to(torch.long)\n",
    "col = torch.from_numpy(adj.col.astype(np.int64)).to(torch.long)\n",
    "edge_index = torch.stack([row, col], dim=0)\n",
    "\n",
    "\n",
    "class TwitterDataset(InMemoryDataset):\n",
    "    def __init__(self, transform=None):\n",
    "        super(TwitterDataset, self).__init__('.', transform, None, None)\n",
    "\n",
    "        data = Data(edge_index=edge_index)\n",
    "        \n",
    "        data.num_nodes = g.number_of_nodes()\n",
    "        \n",
    "        # Features \n",
    "        data.x = torch.from_numpy(features).type(torch.float32)\n",
    "        \n",
    "        # Etiquetas\n",
    "        y = torch.from_numpy(labels).type(torch.long)\n",
    "        data.y = y.clone().detach()\n",
    "        \n",
    "        data.num_classes = 2\n",
    "        \n",
    "        n_nodes = g.number_of_nodes()\n",
    "        \n",
    "        # create train and test masks for data\n",
    "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        train_mask[train_idx] = True\n",
    "        data['train_mask'] = train_mask\n",
    "\n",
    "        self.data, self.slices = self.collate([data])\n",
    "\n",
    "    def _download(self):\n",
    "        return\n",
    "\n",
    "    def _process(self):\n",
    "        return\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}()'.format(self.__class__.__name__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:graphML] *",
   "language": "python",
   "name": "conda-env-graphML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
